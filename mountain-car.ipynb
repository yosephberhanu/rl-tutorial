{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car\n",
    "\n",
    "Q Learning to play Mountain Car, using q table. Based on the tutorial by  sentdex [here](https://www.youtube.com/watch?v=yMk_XtIEzH8&list=PLQVvvaa0QuDezJFIOU5wDdfy4e9vdnx-7). Code updated from the tutorial to comply with changes in the latest version of OpenAI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 3)\n",
      "[[[-7.53240742e-01 -8.15045646e-01 -1.51088096e+00]\n",
      "  [-7.57390838e+00 -4.90005799e+00 -6.85065727e+00]\n",
      "  [-4.28239330e+01 -4.64317950e+01 -4.63791979e+01]\n",
      "  ...\n",
      "  [-1.44497650e+00 -1.27341686e-01 -1.74576352e+00]\n",
      "  [-4.65108777e-01 -8.95860805e-02 -1.71996091e+00]\n",
      "  [-1.24931005e-01 -1.88223196e+00 -7.70006183e-01]]\n",
      "\n",
      " [[-8.69083502e-01 -1.79682836e+00 -1.04324814e+00]\n",
      "  [-3.80901702e+01 -3.75474884e+01 -3.80009233e+01]\n",
      "  [-4.71482686e+01 -4.70927041e+01 -4.54978956e+01]\n",
      "  ...\n",
      "  [-1.68907936e+00 -1.15445862e+00 -1.78680844e+00]\n",
      "  [-9.87051365e-01 -1.16424217e+00 -1.16226603e+00]\n",
      "  [-3.03265671e-01 -5.95276882e-01 -1.56838826e+00]]\n",
      "\n",
      " [[-1.14501971e+00 -1.09670117e+00 -1.46494966e+00]\n",
      "  [-4.07605171e+01 -4.15748579e+01 -4.19538369e+01]\n",
      "  [-4.86832205e+01 -4.80779995e+01 -4.87174953e+01]\n",
      "  ...\n",
      "  [-2.92553078e-01 -1.57994165e-02 -5.58328177e-01]\n",
      "  [-4.07022288e-01 -1.50811461e+00 -6.83982275e-01]\n",
      "  [-4.68175456e-01 -1.64706724e+00 -1.91034396e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-9.07669691e-01 -1.66283871e-01 -2.54932998e-01]\n",
      "  [-3.73045989e-01 -1.63205892e+00 -1.86523563e+00]\n",
      "  [-1.12832657e+00 -1.82640330e+00 -1.38019099e+00]\n",
      "  ...\n",
      "  [-3.60554782e-01 -1.74764360e+00 -6.12934886e-01]\n",
      "  [-8.82796449e-01 -1.00797498e+00 -6.06859573e-01]\n",
      "  [-1.46897767e+00 -3.53855765e-01 -3.78290443e-01]]\n",
      "\n",
      " [[-1.84382197e+00 -6.33258507e-01 -1.93058061e+00]\n",
      "  [-1.04318129e+00 -4.92564401e-01 -1.14659228e+00]\n",
      "  [-1.40054100e+00 -1.47980294e+00 -3.93745692e-01]\n",
      "  ...\n",
      "  [-1.67195538e+00 -6.08188409e-02 -1.53330426e+00]\n",
      "  [-1.78405125e+00 -1.56540123e+00 -4.71934439e-01]\n",
      "  [-4.28378924e-01 -1.26890760e+00 -2.55782940e-01]]\n",
      "\n",
      " [[-1.05027643e+00 -1.98222028e+00 -1.24609874e-01]\n",
      "  [-9.48610829e-01 -4.84378063e-01 -6.13227037e-01]\n",
      "  [-1.37203243e+00 -1.59429323e+00 -1.25727556e+00]\n",
      "  ...\n",
      "  [-1.01461952e+00 -1.26089805e+00 -1.73245685e+00]\n",
      "  [-6.27891722e-01 -2.79006690e-01 -4.28411046e-01]\n",
      "  [-1.28699413e+00 -1.49657924e+00 -1.82581459e+00]]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "np.bool = np.bool_\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\",render_mode='human')\n",
    "#Discretize the observation space to make it manageable \n",
    "DISCRETE_OS_SIZE = [20] * len((env.observation_space.high))\n",
    "\n",
    "# Just like any DL\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# A measure of how future rewards are valued \n",
    "DISCOUNT = 0.95\n",
    "\n",
    "EPISODES = 5000 #0\n",
    "\n",
    "epsilon = 1\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES//2\n",
    "\n",
    "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "show_every = 100\n",
    "discrete_obs_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
    "q_table_file = \"mountain_car_q_table.pkl\"\n",
    "if os.path.isfile(q_table_file):\n",
    "    with open(q_table_file, \"rb\") as fp:   #Pickling\n",
    "        q_table = pickle.load(fp)\n",
    "else:\n",
    "    q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "print(q_table.shape)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discerete_state(state):\n",
    "    discrete_state = (state-env.observation_space.low)/discrete_obs_win_size\n",
    "    return tuple(discrete_state.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 10)\n"
     ]
    }
   ],
   "source": [
    "obs,_ = env.reset()\n",
    "ds = get_discerete_state(obs)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made it on episode 887\n",
      "Made it on episode 1170\n",
      "Made it on episode 1205\n",
      "Made it on episode 1212\n"
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES):\n",
    "    if episode%show_every==0:\n",
    "        render_mode = \"human\"\n",
    "    else:\n",
    "        render_mode = None\n",
    "    env = gym.make(\"MountainCar-v0\", render_mode=render_mode)\n",
    "    \n",
    "    \n",
    "    init_state,_ = env.reset()\n",
    "    discrete_state = get_discerete_state(init_state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        new_discrete_state = get_discerete_state(obs)\n",
    "        done = (terminated or truncated)\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action, )]\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward * DISCOUNT + max_future_q)\n",
    "            q_table[discrete_state+(action,)] = new_q\n",
    "        elif obs[0] >= env.goal_position:\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "            print(f\"Made it on episode {episode}\")\n",
    "        discrete_state = new_discrete_state\n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "env.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Q-Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(q_table_file, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(q_table, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run the game a few times with the learned Q-Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    env = gym.make(\"MountainCar-v0\", render_mode='human')\n",
    "    init_state, _ = env.reset()\n",
    "    discrete_state = get_discerete_state(init_state)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        new_discrete_state = get_discerete_state(obs)\n",
    "        done = (terminated or truncated)\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action, )]\n",
    "        elif obs[0] >= env.goal_position:\n",
    "            print(f\"Made it on iteration {i}\")\n",
    "        discrete_state = new_discrete_state\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
